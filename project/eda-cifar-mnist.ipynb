{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "def get_transforms(std=0.5, mean=0.5):\n",
    "    return transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((mean,), (std,))\n",
    "    ])\n",
    "    \n",
    "def cut_data(data, first_proportion=0.8):\n",
    "    first_size = int(len(data) * first_proportion)\n",
    "    second_size = len(data) - first_size\n",
    "    first_data, second_data = torch.utils.data.random_split(data, [first_size, second_size])\n",
    "    return first_data, second_data\n",
    "\n",
    "def get_data_MNIST(std=0.5, mean=0.5, train_size=0.8):\n",
    "    train_data = datasets.MNIST(root='data', train=True, transform=get_transforms(std, mean), download=True)\n",
    "    test_data = datasets.MNIST(root='data', train=False, transform=get_transforms(std, mean), download=True)\n",
    "    train_data, val_data = cut_data(train_data, train_size)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def get_data_CIFAR10(std=0.5, mean=0.5, train_size=0.8):\n",
    "    train_data = datasets.CIFAR10(root='data', train=True, transform=get_transforms(std, mean), download=True)\n",
    "    test_data = datasets.CIFAR10(root='data', train=False, transform=get_transforms(std, mean), download=True)\n",
    "    train_data, val_data = cut_data(train_data, train_size)\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def inspect(data):\n",
    "    print(f\"Data type: {type(data)}\")\n",
    "    print(f\"Data length: {len(data)}\")\n",
    "    print(f\"Data shape: {data[0][0].shape}\")\n",
    "    \n",
    "def check_data(train_data, val_data, test_data):\n",
    "    inspect(train_data)\n",
    "    inspect(val_data)\n",
    "    inspect(test_data)\n",
    "    # Total de datos\n",
    "    print(f\"Total data: {len(train_data) + len(val_data) + len(test_data)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: <class 'torch.utils.data.dataset.Subset'>\n",
      "Data length: 48000\n",
      "Data shape: torch.Size([1, 28, 28])\n",
      "Data type: <class 'torch.utils.data.dataset.Subset'>\n",
      "Data length: 12000\n",
      "Data shape: torch.Size([1, 28, 28])\n",
      "Data type: <class 'torchvision.datasets.mnist.MNIST'>\n",
      "Data length: 10000\n",
      "Data shape: torch.Size([1, 28, 28])\n",
      "Total data: 70000\n",
      "\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data type: <class 'torch.utils.data.dataset.Subset'>\n",
      "Data length: 40000\n",
      "Data shape: torch.Size([3, 32, 32])\n",
      "Data type: <class 'torch.utils.data.dataset.Subset'>\n",
      "Data length: 10000\n",
      "Data shape: torch.Size([3, 32, 32])\n",
      "Data type: <class 'torchvision.datasets.cifar.CIFAR10'>\n",
      "Data length: 10000\n",
      "Data shape: torch.Size([3, 32, 32])\n",
      "Total data: 60000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_mnist, val_mnist, test_mnist = get_data_MNIST()\n",
    "check_data(train_mnist, val_mnist, test_mnist)\n",
    "train_cifar10, val_cifar10, test_cifar10 = get_data_CIFAR10()\n",
    "check_data(train_cifar10, val_cifar10, test_cifar10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotear una imagen de ejemplo\n",
    "def plot_image(data, index):\n",
    "    image, label = data[index]\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    # Check rgb or grayscale\n",
    "    if image.shape[0] == 1:\n",
    "        image = image[0]\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        image = image.permute(1, 2, 0)\n",
    "        plt.imshow(image)\n",
    "    plt.title(f\"Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_image(train_mnist, 10)\n",
    "plot_image(train_cifar10, 10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos una capa de convolución\n",
    "\n",
    "class ConvLayer(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding='same'):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.conv(x))\n",
    "    \n",
    "# Max Pooling + Batch Normalization + Dropout (opcional)\n",
    "class MaxPoolBN(torch.nn.Module): \n",
    "    def __init__(self, in_channels, kernel_size = 2, stride=2, padding=0, dropout=0.1):\n",
    "        super(MaxPoolBN, self).__init__()\n",
    "        self.max_pool = torch.nn.MaxPool2d(kernel_size, stride, padding)\n",
    "        self.bn = torch.nn.BatchNorm2d(in_channels)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.bn(self.max_pool(x)))\n",
    "    \n",
    "# Capa densa + dropout (opcional)\n",
    "class Dense(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1):\n",
    "        super(Dense, self).__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.dropout(self.linear(x))\n",
    "    \n",
    "# structure = [N, L1, k1, ..., Ln, kn], con L la cantidad de capas y k el kernel size\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self,in_channels, structure, size, classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.size = size\n",
    "        self.structure = structure\n",
    "        self.L = [structure[i] for i in range(len(structure)) if i % 2 == 1]\n",
    "        self.k = [structure[i] for i in range(len(structure)) if i % 2 == 0 and i != 0]\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(self.L)):\n",
    "            if i == 0:\n",
    "                self.layers.append(ConvLayer(in_channels, self.L[i], self.k[i]))\n",
    "                self.layers.append(torch.nn.Dropout(0.1))\n",
    "            else:\n",
    "                self.layers.append(ConvLayer(self.L[i-1], self.L[i], self.k[i]))\n",
    "                self.layers.append(torch.nn.Dropout(0.1))\n",
    "                \n",
    "            # Max pooling en cantidad par de capas \n",
    "            if i % 2 == 1:\n",
    "                self.layers.append(MaxPoolBN(self.L[i], 2))\n",
    "                self.size = self.size // 2\n",
    "                \n",
    "        # Caso impares\n",
    "        if len(self.L) % 2 == 1:\n",
    "            self.layers.append(MaxPoolBN(self.L[-1], 2))\n",
    "            self.size = self.size // 2\n",
    "            \n",
    "        # Cabeza clasificadora\n",
    "        self.layers.append(torch.nn.Flatten())\n",
    "        self.layers.append(Dense(self.L[-1] * self.size * self.size, 128))\n",
    "        self.layers.append(torch.nn.Dropout(0.5))\n",
    "        self.layers.append(Dense(128, classes))\n",
    "        if self.size == 0:\n",
    "            self.size=1       \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "# Profundidad de CIFAR10 (32/2^4 = 2 -> 4x2 capas convolucionales maximo) depth in [2,8]\n",
    "# E:32 --c1--> 32 --c2--> 32 --MP-->\n",
    "# 16 --c3--> 16 --c4--> 16 --MP-->\n",
    "# 8 --c5--> 8 --c6--> 8 --MP-->\n",
    "# 4 --c7--> 4 --c8--> 4 --MP--> 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Probar con un batch de entrada 1 x 1 x 28 x 28\n",
    "# Cargamos el dataset de CIFAR10\n",
    "train_data, val_data, test_data = get_data_CIFAR10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 3, 32, 32])\n",
      "ConvNet(\n",
      "  (layers): ModuleList(\n",
      "    (0): ConvLayer(\n",
      "      (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): ConvLayer(\n",
      "      (conv): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): Dropout(p=0.1, inplace=False)\n",
      "    (4): MaxPoolBN(\n",
      "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (5): ConvLayer(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (6): Dropout(p=0.1, inplace=False)\n",
      "    (7): MaxPoolBN(\n",
      "      (max_pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Dense(\n",
      "      (linear): Linear(in_features=4096, out_features=128, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (10): Dropout(p=0.5, inplace=False)\n",
      "    (11): Dense(\n",
      "      (linear): Linear(in_features=128, out_features=10, bias=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Creamos el modelo\n",
    "# L = [64, 64, 128, 128, 128, 128, 64, 64]\n",
    "# k = [ 3,  3,   3,   3,   3,   3,  3,  3]\n",
    "# structure = [item for sublist in [[L[i], k[i]] for i in range(len(L)) ] for item in sublist]\n",
    "# structure = [len(structure)//2] + structure\n",
    "structure = [3, 16, 3, 32, 5, 64, 3]\n",
    "\n",
    "# Obtener un ejemplo de entrada\n",
    "x, y = train_data[0]\n",
    "x = x.unsqueeze(0)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "# Creamos el modelo\n",
    "model = ConvNet(3, structure, size=32, classes=10)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 16, 3, 32, 5, 64, 3]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos el modelo\n",
    "y_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model, data_loader, criterion, optimizer, device, train=True):\n",
    "    if train:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    \n",
    "    loss_total = 0\n",
    "    acc_total = 0\n",
    "    for x, y in data_loader:\n",
    "        # Enviamos los datos al dispositivo\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        # Forward\n",
    "        with torch.set_grad_enabled(train):\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "        # Backward\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Acumulamos los resultados\n",
    "        loss_total += loss.item() * x.size(0)\n",
    "        acc_total += (y_pred.argmax(1) == y).sum().item()\n",
    "    # Promediamos los resultados\n",
    "    loss_total /= len(data_loader.dataset)\n",
    "    acc_total /= len(data_loader.dataset)\n",
    "    return loss_total, acc_total\n",
    "\n",
    "def train_epoch(model, train_loader, val_loader, criterion, optimizer, device):\n",
    "    train_loss, train_acc = run_epoch(model, train_loader, criterion, optimizer, device, train=True)\n",
    "    val_loss, val_acc = run_epoch(model, val_loader, criterion, optimizer, device, train=False)\n",
    "    return train_loss, train_acc, val_loss, val_acc\n",
    "\n",
    "# Entrenar un modelo\n",
    "def train(model, train_data, val_data, epochs=100, batch_size=256, lr=0.001, device='cpu', folder='models'):\n",
    "    # Definimos el optimizador\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    # Definimos la función de costo\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    # Enviamos el modelo al dispositivo\n",
    "    model.to(device)\n",
    "    # Definimos los dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    # Definimos las listas para guardar los resultados\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    # Early stopping\n",
    "    epochs_without_improvement = 0\n",
    "    # Entrenamos\n",
    "    for epoch in range(epochs):\n",
    "        # Entrenamos un epoch\n",
    "        train_loss, train_acc, val_loss, val_acc = train_epoch(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "        # Guardamos los resultados\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        # Si mejora el resultado, guardamos el modelo\n",
    "        best_val_loss = np.min(val_losses)\n",
    "        if val_loss > best_val_loss:\n",
    "            torch.save(model.state_dict(), f\"{folder}/model.pth\")\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "        # Si no mejora el resultado durante 3 epochs, detenemos el entrenamiento\n",
    "        if epochs_without_improvement == 3:\n",
    "            print(f\"Training stopped at epoch {epoch+1}\")\n",
    "            break\n",
    "        # Imprimimos los resultados\n",
    "        print(f\"Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Train loss: {train_loss:.4f} - Train acc: {train_acc:.4f}\")\n",
    "        print(f\"Val loss: {val_loss:.4f} - Val acc: {val_acc:.4f}\")\n",
    "    return train_losses, train_accuracies, val_losses, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:\n",
      "Train loss: 1.6145 - Train acc: 0.4406\n",
      "Val loss: 1.2380 - Val acc: 0.5604\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos el modelo\n",
    "train_losses, train_accuracies, val_losses, val_accuracies = train(model, train_data, val_data, epochs=1, batch_size=256, lr=0.001, device='cuda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm 1 Proposed Genetic Algorithm\n",
    "Input: (max_gen, cross_prob, mutation_prob,\n",
    "max_pop, conv_parameters)\n",
    "Output: elite\n",
    "\n",
    "\n",
    "1: generation ← 0\n",
    "\n",
    "2: population ← Generate initial population from Input parameters.\n",
    "\n",
    "3: while generation < max_gen do\n",
    "\n",
    "4: for individual in population do\n",
    "\n",
    "5: individual.accuracy ← obtain validation accuracy value from training network defined by individual’s cromosome and conv_parameters\n",
    "\n",
    "6: end for\n",
    "\n",
    "7: Calculate each individual fitness from population\n",
    "\n",
    "8: elite ← Get individual with best fitness from population\n",
    "\n",
    "9: children_list ← Empty list\n",
    "\n",
    "10: next_pop ← Empty list\n",
    "\n",
    "11: for individuals in population do\n",
    "\n",
    "12: if Random([0,1]) ≤cross_prob then\n",
    "\n",
    "13: Choose parent1 and parent2 from population using proportional roulette wheel selection.\n",
    "\n",
    "14: if generation/max_gen < Random([0,1]) then\n",
    "\n",
    "15: child1, child2 ← Cross parent1 and parent2 sequentially\n",
    "\n",
    "16: else\n",
    "\n",
    "17: child1, child2 ← Cross parent1 and parent2 with binary list\n",
    "\n",
    "18: end if\n",
    "\n",
    "19: Add child1 and child2 to children_list\n",
    "\n",
    "20: end if\n",
    "\n",
    "21: if generation < max_gen/2 then\n",
    "\n",
    "22: next_pop ← apply mutation to children_list with probability mutation_prob\n",
    "\n",
    "23: else\n",
    "\n",
    "24: survivors ← Select a fraction of survivors from population, according to fitness\n",
    "\n",
    "25: Add survivors, children_list and elite to next_pop\n",
    "\n",
    "26: next_pop ← apply mutation to next_pop with probability mutation_prob\n",
    "\n",
    "27: end if\n",
    "\n",
    "28: Fill next_pop with random individuals or delete random individuals until max_pop total individuals\n",
    "\n",
    "29: if elite not in next_pop then\n",
    "\n",
    "30: Replace random individual from next_pop with elite\n",
    "\n",
    "31: end if\n",
    "\n",
    "32: population ← next_population\n",
    "\n",
    "33: generation ← generation +1\n",
    "\n",
    "34: end for\n",
    "\n",
    "35: end while\n",
    "\n",
    "36: Train population and return the elite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cromosome_to_model(cromosome, in_channels, size, classes):\n",
    "    # Creamos el modelo\n",
    "    model = ConvNet(in_channels, cromosome, size=size, classes=classes)\n",
    "    return model\n",
    "\n",
    "def mutate_cromosome(cromosome, depths, filters, kernel_sizes):\n",
    "    # Rand 0.5\n",
    "    if np.random.rand() < 0.5:\n",
    "        current_depth = cromosome[0]\n",
    "        # Mutamos la cantidad de capas eligiendo un valor al azar del vector depths\n",
    "        new_depth = np.random.choice(depths)\n",
    "        if new_depth >= current_depth:\n",
    "            for i in range(new_depth - current_depth): \n",
    "                cromosome.append(np.random.choice(filters))\n",
    "                cromosome.append(np.random.choice(kernel_sizes))\n",
    "                \n",
    "        elif new_depth < current_depth:\n",
    "            for i in range(current_depth - new_depth):\n",
    "                # Eliminamos el filtro y el kernel size\n",
    "                cromosome.pop()\n",
    "                cromosome.pop()\n",
    "                \n",
    "    else: # Rand 0.5\n",
    "        # Mutamos una posición al azar que no sea la primera\n",
    "        pos = np.random.randint(1, len(cromosome))\n",
    "        # Si es impar, mutamos el filtro, si es par, mutamos el kernel size\n",
    "        cromosome[pos] = np.random.choice(filters) if pos % 2 == 1 else np.random.choice(kernel_sizes)\n",
    "    \n",
    "    return cromosome\n",
    "\n",
    "def sequential_crossover(cromosome1, cromosome2):\n",
    "    # Elegimos un punto de corte al azar\n",
    "    pos = np.random.randint(1, len(min(cromosome1[1:], cromosome2[1:]))//2 + 1)\n",
    "    # Realizamos el crossover\n",
    "    new_cromosome1 = cromosome1[:2*pos+1] + cromosome2[2*pos+1:]\n",
    "    new_cromosome2 = cromosome2[:2*pos+1] + cromosome1[2*pos+1:]\n",
    "    # Actualizamos la profundidad\n",
    "    new_cromosome1[0] = len(new_cromosome1[1:])//2\n",
    "    new_cromosome2[0] = len(new_cromosome2[1:])//2\n",
    "    return new_cromosome1, new_cromosome2\n",
    "\n",
    "def binary_crossover(cromosome1, cromosome2):\n",
    "    # Lista binaria de len(cromosome)-1\n",
    "    binary_list = [np.random.randint(0, 2) for _ in range(len(min(cromosome1[1:], cromosome2[1:])))]\n",
    "    # Realizamos el crossover\n",
    "    for i in range(len(binary_list)):\n",
    "        if binary_list[i] == 1:\n",
    "            cromosome1[i+1], cromosome2[i+1] = cromosome2[i+1], cromosome1[i+1]\n",
    "    return cromosome1, cromosome2\n",
    "\n",
    "def crossover(cromosome1, cromosome2, seq_prob=0.6): # seq prob = #gen/#total gen\n",
    "    # Elegimos el tipo de crossover\n",
    "    if np.random.rand() < seq_prob:\n",
    "        return sequential_crossover(cromosome1, cromosome2)\n",
    "    else:\n",
    "        return binary_crossover(cromosome1, cromosome2)\n",
    "\n",
    "def evaluate_cromosome(cromosome, train_data, val_data, in_channels, size, classes, epochs=100, batch_size=256, lr=0.001, device='cpu', folder='models'):\n",
    "    # Creamos el modelo\n",
    "    model = cromosome_to_model(cromosome, in_channels, size, classes)\n",
    "    # Entrenamos el modelo\n",
    "    train_losses, train_accuracies, val_losses, val_accuracies = train(model, train_data, val_data, epochs=epochs, batch_size=batch_size, lr=lr, device=device, folder=folder)\n",
    "    return max(val_accuracies)\n",
    "\n",
    "def generate_population(depths, filters, kernel_sizes, population_size):\n",
    "    population = []\n",
    "    for i in range(population_size):\n",
    "        # Elegimos la profundidad\n",
    "        depth = np.random.choice(depths)\n",
    "        # Elegimos los filtros y los kernel sizes\n",
    "        filters = np.random.choice(filters, size=depth)\n",
    "        kernel_sizes = np.random.choice(kernel_sizes, size=depth)\n",
    "        # Creamos el cromosoma\n",
    "        cromosome = [depth]\n",
    "        for j in range(depth):\n",
    "            cromosome.append(filters[j])\n",
    "            cromosome.append(kernel_sizes[j])\n",
    "        population.append(cromosome)\n",
    "    return population\n",
    "\n",
    "def genetic_algorithm(train_data, val_data, in_channels, size, classes, epochs=1, batch_size=256, lr=0.001, device='cpu', folder='models',\n",
    "                       population_size=10, generations=10, depths=[1, 2, 3, 4, 5], filters=[16, 32, 64, 128, 256], kernel_sizes=[3, 5]):\n",
    "    # Inicializamos la población\n",
    "    population = generate_population(depths, filters, kernel_sizes, population_size)\n",
    "    # Entrenamos la población\n",
    "    for generation in range(generations):\n",
    "        cross_prob = generation/generations\n",
    "        mutation_prob = 1 - cross_prob\n",
    "        print(f\"Generation {generation+1}/{generations}:\")\n",
    "        # Evaluamos cada cromosoma\n",
    "        val_accuracy = []\n",
    "        for idx, cromosome in enumerate(population):\n",
    "            val_accuracy += [evaluate_cromosome(cromosome, train_data, val_data, in_channels, size, classes, epochs=epochs, batch_size=batch_size, lr=lr, device=device, folder=folder)]\n",
    "            print(f\"Validation Accuracy - Cromosome{idx+1}: {val_accuracy[-1]}\")\n",
    "            \n",
    "        if generation < generations//2:\n",
    "            # Fitness al comienzo diferencia los mejores de los peores\n",
    "            fitness = np.array(val_accuracy)/sum(val_accuracy)\n",
    "        else:\n",
    "            # Ranking es mejor al ser similares\n",
    "            # Best accuracy -> Rank 1, ..., Worst accuracy -> Rank #population_size\n",
    "            ranking = np.argsort(val_accuracy)+1\n",
    "            # Calculamos el fitness\n",
    "            fitness = (len(ranking)+1-ranking)/sum(len(ranking)+1-ranking)\n",
    "        # Seleccionamos el mejor cromosoma\n",
    "        elite = population[np.argmax(val_accuracy)]\n",
    "\n",
    "        # Creamos los hijos \n",
    "        children = []\n",
    "        # Creamos la nueva población\n",
    "        next_population = []\n",
    "        # Recorremos la población\n",
    "        for cromosome in population:\n",
    "            # Elegimos los padres con probabilidad cross_prob\n",
    "            if np.random.rand() < cross_prob:\n",
    "                # Elegimos padres con proportional roulette wheel selection\n",
    "                parent1 = population[np.random.choice(len(population), p=fitness)]\n",
    "                parent2 = population[np.random.choice(len(population), p=fitness)]\n",
    "                if generation/generations < np.random.rand():\n",
    "                    # Crossover secuencial\n",
    "                    child1, child2 = sequential_crossover(parent1, parent2)\n",
    "                else:\n",
    "                    # Crossover binario\n",
    "                    child1, child2 = binary_crossover(parent1, parent2)\n",
    "                # Agrergamos los hijos a la lista\n",
    "                children.append(child1)\n",
    "                children.append(child2)\n",
    "            if generation < generations/2:\n",
    "                # Mutamos con probabilidad mutation_prob\n",
    "                if np.random.rand() < mutation_prob:\n",
    "                    # Mutamos el cromosoma\n",
    "                    cromosome = mutate_cromosome(cromosome, depths, filters, kernel_sizes)\n",
    "                    # Agregamos el cromosoma mutado a la lista\n",
    "                    children.append(cromosome)\n",
    "            else:\n",
    "                # Agregamos los hijos + elite a la nueva población\n",
    "                next_population.append(elite)\n",
    "                next_population += children\n",
    "                # Rellenamos la población con sobrevivientes de mejor fitness hasta completar 0.5 de la población\n",
    "                cumulated_fitness = 0\n",
    "                survivors = []\n",
    "                while cumulated_fitness < 0.5:\n",
    "                    # Elegimos al de mejor fitness\n",
    "                    survivor = population[np.argmax(fitness)]\n",
    "                    # Lo agregamos a la lista de sobrevivientes\n",
    "                    survivors.append(survivor)\n",
    "                    # Lo eliminamos de la población\n",
    "                    population.remove(survivor)\n",
    "                    # Actualizamos el fitness acumulado\n",
    "                    cumulated_fitness += fitness[np.argmax(fitness)]\n",
    "                # Agregamos los sobrevivientes a la nueva población\n",
    "                next_population += survivors\n",
    "                \n",
    "                # Aplicamos mutación a la nueva población con probabilidad mutation_prob\n",
    "                for i in range(len(next_population)):\n",
    "                    if np.random.rand() < mutation_prob:\n",
    "                        # Mutamos el cromosoma\n",
    "                        next_population[i] = mutate_cromosome(next_population[i], depths, filters, kernel_sizes)\n",
    "                # Rellenamos si es necesario\n",
    "                if len(next_population) < population_size:\n",
    "                    # Rellenamos con cromosomas aleatorios\n",
    "                    next_population += generate_population(depths, filters, kernel_sizes, population_size-len(next_population))\n",
    "                # Si sobran cromosomas, nos quedamos solo con los que necesitamos\n",
    "                if len(next_population) > population_size: \n",
    "                    indexes = np.random.choice(range(len(next_population)), size=population_size, replace=False)\n",
    "                    next_population = [next_population[idx] for idx in indexes]\n",
    "                # Si no esta el elite, lo agregamos\n",
    "                if elite not in next_population:\n",
    "                    # Reemplazamos un cromosoma aleatorio por el elite\n",
    "                    next_population[np.random.choice(len(next_population))] = elite\n",
    "                # Actualizamos la población\n",
    "                population = next_population\n",
    "    # Evaluamos la última generación\n",
    "    fitness = [evaluate_cromosome(cromosome,\n",
    "                                  train_data,\n",
    "                                  val_data,\n",
    "                                  in_channels,\n",
    "                                  size,\n",
    "                                  classes,\n",
    "                                  epochs=epochs,\n",
    "                                  batch_size=batch_size,\n",
    "                                  lr=lr,\n",
    "                                  device=device,\n",
    "                                  folder=f\"{folder}/best_generation{idx+1}\") for idx, cromosome in enumerate(population)]\n",
    "    # Seleccionamos el mejor cromosoma\n",
    "    best_cromosome = population[np.argmax(fitness)]\n",
    "    return best_cromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 1/3:\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7857 - Train acc: 0.3449\n",
      "Val loss: 1.5992 - Val acc: 0.4089\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4154 - Train acc: 0.4929\n",
      "Val loss: 1.2341 - Val acc: 0.5680\n",
      "Validation Accuracy - Cromosome1: 0.568\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7779 - Train acc: 0.3474\n",
      "Val loss: 1.5712 - Val acc: 0.4354\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4534 - Train acc: 0.4769\n",
      "Val loss: 1.4104 - Val acc: 0.4939\n",
      "Validation Accuracy - Cromosome2: 0.4939\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7699 - Train acc: 0.3509\n",
      "Val loss: 1.5612 - Val acc: 0.4366\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3812 - Train acc: 0.5047\n",
      "Val loss: 1.3137 - Val acc: 0.5415\n",
      "Validation Accuracy - Cromosome3: 0.5415\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7215 - Train acc: 0.4225\n",
      "Val loss: 1.3752 - Val acc: 0.5282\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2796 - Train acc: 0.5548\n",
      "Val loss: 1.1071 - Val acc: 0.6073\n",
      "Validation Accuracy - Cromosome4: 0.6073\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6056 - Train acc: 0.4324\n",
      "Val loss: 1.2655 - Val acc: 0.5476\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2504 - Train acc: 0.5621\n",
      "Val loss: 1.1390 - Val acc: 0.6039\n",
      "Validation Accuracy - Cromosome5: 0.6039\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7118 - Train acc: 0.3838\n",
      "Val loss: 1.5636 - Val acc: 0.4537\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3659 - Train acc: 0.5170\n",
      "Val loss: 1.1239 - Val acc: 0.6100\n",
      "Validation Accuracy - Cromosome6: 0.61\n",
      "Epoch 1/2:\n",
      "Train loss: 1.5926 - Train acc: 0.4402\n",
      "Val loss: 1.3562 - Val acc: 0.5282\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2455 - Train acc: 0.5638\n",
      "Val loss: 1.0660 - Val acc: 0.6293\n",
      "Validation Accuracy - Cromosome7: 0.6293\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7907 - Train acc: 0.3458\n",
      "Val loss: 1.6522 - Val acc: 0.3927\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4296 - Train acc: 0.4886\n",
      "Val loss: 1.3889 - Val acc: 0.5085\n",
      "Validation Accuracy - Cromosome8: 0.5085\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7113 - Train acc: 0.3855\n",
      "Val loss: 1.5246 - Val acc: 0.4658\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3699 - Train acc: 0.5165\n",
      "Val loss: 1.2164 - Val acc: 0.5851\n",
      "Validation Accuracy - Cromosome9: 0.5851\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7103 - Train acc: 0.3810\n",
      "Val loss: 1.5709 - Val acc: 0.4331\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3879 - Train acc: 0.5051\n",
      "Val loss: 1.2579 - Val acc: 0.5613\n",
      "Validation Accuracy - Cromosome10: 0.5613\n",
      "Generation 2/3:\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7774 - Train acc: 0.3474\n",
      "Val loss: 1.6272 - Val acc: 0.4148\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4109 - Train acc: 0.4924\n",
      "Val loss: 1.2114 - Val acc: 0.5710\n",
      "Validation Accuracy - Cromosome1: 0.571\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7915 - Train acc: 0.3437\n",
      "Val loss: 1.6512 - Val acc: 0.3765\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4355 - Train acc: 0.4863\n",
      "Val loss: 1.2917 - Val acc: 0.5456\n",
      "Validation Accuracy - Cromosome2: 0.5456\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7544 - Train acc: 0.3613\n",
      "Val loss: 1.5637 - Val acc: 0.4368\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3989 - Train acc: 0.5015\n",
      "Val loss: 1.2293 - Val acc: 0.5680\n",
      "Validation Accuracy - Cromosome3: 0.568\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6340 - Train acc: 0.4237\n",
      "Val loss: 1.3279 - Val acc: 0.5333\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2519 - Train acc: 0.5633\n",
      "Val loss: 1.0221 - Val acc: 0.6425\n",
      "Validation Accuracy - Cromosome4: 0.6425\n",
      "Epoch 1/2:\n",
      "Train loss: 1.5849 - Train acc: 0.4431\n",
      "Val loss: 1.2628 - Val acc: 0.5613\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2093 - Train acc: 0.5738\n",
      "Val loss: 1.0748 - Val acc: 0.6354\n",
      "Validation Accuracy - Cromosome5: 0.6354\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7024 - Train acc: 0.3896\n",
      "Val loss: 1.4185 - Val acc: 0.4904\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3544 - Train acc: 0.5202\n",
      "Val loss: 1.1813 - Val acc: 0.5871\n",
      "Validation Accuracy - Cromosome6: 0.5871\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6230 - Train acc: 0.4290\n",
      "Val loss: 1.3278 - Val acc: 0.5430\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2636 - Train acc: 0.5580\n",
      "Val loss: 1.0947 - Val acc: 0.6131\n",
      "Validation Accuracy - Cromosome7: 0.6131\n",
      "Epoch 1/2:\n",
      "Train loss: 1.8121 - Train acc: 0.3339\n",
      "Val loss: 1.6943 - Val acc: 0.3827\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4828 - Train acc: 0.4688\n",
      "Val loss: 1.3934 - Val acc: 0.5005\n",
      "Validation Accuracy - Cromosome8: 0.5005\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6978 - Train acc: 0.3928\n",
      "Val loss: 1.4725 - Val acc: 0.4636\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3513 - Train acc: 0.5200\n",
      "Val loss: 1.2280 - Val acc: 0.5622\n",
      "Validation Accuracy - Cromosome9: 0.5622\n",
      "Epoch 1/2:\n",
      "Train loss: 1.8471 - Train acc: 0.3232\n",
      "Val loss: 1.8099 - Val acc: 0.3261\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4941 - Train acc: 0.4692\n",
      "Val loss: 1.3813 - Val acc: 0.5080\n",
      "Validation Accuracy - Cromosome10: 0.508\n",
      "Generation 3/3:\n",
      "Epoch 1/2:\n",
      "Train loss: 1.8077 - Train acc: 0.3408\n",
      "Val loss: 1.6421 - Val acc: 0.3838\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4009 - Train acc: 0.4997\n",
      "Val loss: 1.2212 - Val acc: 0.5587\n",
      "Validation Accuracy - Cromosome1: 0.5587\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7111 - Train acc: 0.3819\n",
      "Val loss: 1.5658 - Val acc: 0.4495\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3738 - Train acc: 0.5126\n",
      "Val loss: 1.2504 - Val acc: 0.5670\n",
      "Validation Accuracy - Cromosome2: 0.567\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7866 - Train acc: 0.3500\n",
      "Val loss: 1.6260 - Val acc: 0.4158\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4271 - Train acc: 0.4955\n",
      "Val loss: 1.2191 - Val acc: 0.5636\n",
      "Validation Accuracy - Cromosome3: 0.5636\n",
      "Epoch 1/2:\n",
      "Train loss: 1.5830 - Train acc: 0.4450\n",
      "Val loss: 1.2695 - Val acc: 0.5652\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2113 - Train acc: 0.5781\n",
      "Val loss: 1.0007 - Val acc: 0.6511\n",
      "Validation Accuracy - Cromosome4: 0.6511\n",
      "Epoch 1/2:\n",
      "Train loss: 2.1215 - Train acc: 0.4221\n",
      "Val loss: 1.2716 - Val acc: 0.5506\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2656 - Train acc: 0.5647\n",
      "Val loss: 1.0773 - Val acc: 0.6280\n",
      "Validation Accuracy - Cromosome5: 0.628\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7071 - Train acc: 0.3856\n",
      "Val loss: 1.5281 - Val acc: 0.4413\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3576 - Train acc: 0.5219\n",
      "Val loss: 1.2838 - Val acc: 0.5491\n",
      "Validation Accuracy - Cromosome6: 0.5491\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6557 - Train acc: 0.4030\n",
      "Val loss: 1.3984 - Val acc: 0.5147\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2425 - Train acc: 0.5615\n",
      "Val loss: 1.0291 - Val acc: 0.6320\n",
      "Validation Accuracy - Cromosome7: 0.632\n",
      "Epoch 1/2:\n",
      "Train loss: 1.6679 - Train acc: 0.4379\n",
      "Val loss: 1.2271 - Val acc: 0.5659\n",
      "Epoch 2/2:\n",
      "Train loss: 1.2562 - Train acc: 0.5620\n",
      "Val loss: 1.1971 - Val acc: 0.5734\n",
      "Validation Accuracy - Cromosome8: 0.5734\n",
      "Epoch 1/2:\n",
      "Train loss: 1.7062 - Train acc: 0.3897\n",
      "Val loss: 1.5634 - Val acc: 0.4297\n",
      "Epoch 2/2:\n",
      "Train loss: 1.3718 - Train acc: 0.5154\n",
      "Val loss: 1.2590 - Val acc: 0.5639\n",
      "Validation Accuracy - Cromosome9: 0.5639\n",
      "Epoch 1/2:\n",
      "Train loss: 1.8234 - Train acc: 0.3347\n",
      "Val loss: 1.6219 - Val acc: 0.4139\n",
      "Epoch 2/2:\n",
      "Train loss: 1.4733 - Train acc: 0.4753\n",
      "Val loss: 1.3654 - Val acc: 0.5157\n",
      "Validation Accuracy - Cromosome10: 0.5157\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ignacio\\Google Drive\\Universidad\\2023 I Semestre de Otoño\\EL7007 I Introduccion al procesamiento digital de imagenes\\Genetic-Algorithms\\project\\eda-cifar-mnist.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Entrenar el modelo\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m best_cromosome \u001b[39m=\u001b[39m genetic_algorithm(train_data,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                     val_data,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                     in_channels \u001b[39m=\u001b[39;49m \u001b[39m3\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                     size \u001b[39m=\u001b[39;49m \u001b[39m32\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                     classes \u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                     epochs \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                                     batch_size \u001b[39m=\u001b[39;49m \u001b[39m256\u001b[39;49m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                     lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                                     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m                                     folder\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m                                     population_size\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                                     generations\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m                                     depths\u001b[39m=\u001b[39;49m[\u001b[39m2\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m4\u001b[39;49m, \u001b[39m5\u001b[39;49m, \u001b[39m6\u001b[39;49m, \u001b[39m7\u001b[39;49m, \u001b[39m8\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                                     filters\u001b[39m=\u001b[39;49m[\u001b[39m16\u001b[39;49m, \u001b[39m32\u001b[39;49m, \u001b[39m64\u001b[39;49m, \u001b[39m128\u001b[39;49m, \u001b[39m256\u001b[39;49m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m                                     kernel_sizes\u001b[39m=\u001b[39;49m[\u001b[39m3\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\Ignacio\\Google Drive\\Universidad\\2023 I Semestre de Otoño\\EL7007 I Introduccion al procesamiento digital de imagenes\\Genetic-Algorithms\\project\\eda-cifar-mnist.ipynb Cell 14\u001b[0m in \u001b[0;36mgenetic_algorithm\u001b[1;34m(train_data, val_data, in_channels, size, classes, epochs, batch_size, lr, device, folder, population_size, generations, depths, filters, kernel_sizes)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m     child1, child2 \u001b[39m=\u001b[39m sequential_crossover(parent1, parent2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m     \u001b[39m# Crossover binario\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     child1, child2 \u001b[39m=\u001b[39m binary_crossover(parent1, parent2)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m \u001b[39m# Agrergamos los hijos a la lista\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m children\u001b[39m.\u001b[39mappend(child1)\n",
      "\u001b[1;32mc:\\Users\\Ignacio\\Google Drive\\Universidad\\2023 I Semestre de Otoño\\EL7007 I Introduccion al procesamiento digital de imagenes\\Genetic-Algorithms\\project\\eda-cifar-mnist.ipynb Cell 14\u001b[0m in \u001b[0;36mbinary_crossover\u001b[1;34m(cromosome1, cromosome2)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(binary_list)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mif\u001b[39;00m binary_list[i] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         cromosome1[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], cromosome2[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m cromosome2[i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m], cromosome1[i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Ignacio/Google%20Drive/Universidad/2023%20I%20Semestre%20de%20Oto%C3%B1o/EL7007%20I%20Introduccion%20al%20procesamiento%20digital%20de%20imagenes/Genetic-Algorithms/project/eda-cifar-mnist.ipynb#X16sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mreturn\u001b[39;00m cromosome1, cromosome2\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "best_cromosome = genetic_algorithm(train_data,\n",
    "                                    val_data,\n",
    "                                    in_channels = 3,\n",
    "                                    size = 32,\n",
    "                                    classes = 10,\n",
    "                                    epochs = 2,\n",
    "                                    batch_size = 256,\n",
    "                                    lr=0.001,\n",
    "                                    device='cuda',\n",
    "                                    folder='models',\n",
    "                                    population_size=10,\n",
    "                                    generations=3,\n",
    "                                    depths=[2, 3, 4, 5, 6, 7, 8],\n",
    "                                    filters=[16, 32, 64, 128, 256],\n",
    "                                    kernel_sizes=[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 64, 3, 64, 3, 64, 3, 64, 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cromosome"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
